{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTATION DES LIBRAIRIES ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from neattext import functions\n",
    "from wordcloud import WordCloud\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('vader_lexicon')\n",
    "# ! python -m spacy download fr_core_news_sm\n",
    "# ! pip install pyLDAvis\n",
    "# ! pip install bertopic\n",
    "# ! pip install neattext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECUPERATION DES DONNEES ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./Samples/dataset_police_nationale.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAITEMENT DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de récupération des dates d'au plus 2 ans\n",
    "def select_dates():\n",
    "    dataset[\"Dates\"] = dataset[\"Dates\"].apply(lambda date: date.replace(\"un\", \"1\"))\n",
    "    indices_to_keep = []\n",
    "    indices_to_remove = []\n",
    "    \n",
    "    for indice in range(len(dataset[\"Dates\"])):\n",
    "        if \"jours\" in dataset[\"Dates\"][indice] or \"semaine\" in dataset[\"Dates\"][indice] or \"mois\" in dataset[\"Dates\"][indice]:\n",
    "            indices_to_keep.append(indice)\n",
    "        elif \"1 an\" in dataset[\"Dates\"][indice] or \"2 ans\" in dataset[\"Dates\"][indice]:\n",
    "            indices_to_keep.append(indice)\n",
    "            \n",
    "    for elt in range(len(dataset)):\n",
    "        if elt not in indices_to_keep:\n",
    "            indices_to_remove.append(elt)\n",
    "            \n",
    "    dataset.drop(indices_to_remove, axis=0, inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage des données\n",
    "def clean_avis(avis):\n",
    "    stop_words = set(stopwords.words('french')) # Charger les mots vides (stop words)\n",
    "    stop_words.update([\"tout\", \"er\", \"a\", \"h\", \"bien\"])\n",
    "    stop_words.update(stopwords.words('english'))\n",
    "    cleaned_avis = functions.remove_emojis(avis)\n",
    "    cleaned_avis = re.sub(r'\\W+', ' ', cleaned_avis.lower())  # Supprimer les caractères non alphabétiques et convertir en minuscules\n",
    "    cleaned_avis = re.sub(r'\\d+', '', cleaned_avis)  # Supprimer les chiffres\n",
    "    cleaned_avis = ' '.join([word for word in cleaned_avis.split() if word not in stop_words])  # Supprimer les mots vides\n",
    "    return cleaned_avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les données dont les dates n'excèdent pas 2 ans\n",
    "dataset = select_dates()\n",
    "\n",
    "# Nettoyer les avis du jeu de données\n",
    "dataset[\"Processing\"] = dataset[\"Avis\"].apply(clean_avis)\n",
    "\n",
    "# Tokenisation des avis\n",
    "dataset[\"Processing\"] = dataset[\"Processing\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELE CAMEMBERT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle CamemBert\n",
    "\n",
    "analyzer = pipeline(\n",
    "    task='text-classification',\n",
    "    model=\"cmarkea/distilcamembert-base-sentiment\",\n",
    "    tokenizer=\"cmarkea/distilcamembert-base-sentiment\"\n",
    ")\n",
    "\n",
    "comments_list = [dataset[\"Avis\"][elt] for elt in range(len(dataset[\"Avis\"]))]\n",
    "\n",
    "avis_list = []\n",
    "\n",
    "# N'autoriser que 510 caractères par avis\n",
    "for index in range(len(comments_list)):\n",
    "    if len(comments_list[index]) > 510:\n",
    "        phrase = f\"{comments_list[index][:510]}\"\n",
    "    else:\n",
    "        phrase = f\"{comments_list[index]}\"\n",
    "\n",
    "    resultat = analyzer(\n",
    "        phrase,\n",
    "        return_all_scores=True\n",
    "    )\n",
    "\n",
    "    avis_list.append(resultat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des labels\n",
    "score_list = []\n",
    "labels_list = []\n",
    "for index in range(len(avis_list)):\n",
    "    for dict_list in avis_list[index]:\n",
    "        for elt in dict_list:\n",
    "            score_list.append(elt[\"score\"])\n",
    "        for elt in dict_list:\n",
    "            if elt[\"score\"] == max(score_list):\n",
    "                if elt[\"label\"] == \"1 star\" or elt[\"label\"] == \"2 stars\":\n",
    "                    labels_list.append(\"Négatif\")\n",
    "                elif elt[\"label\"] == \"4 stars\" or elt[\"label\"] == \"5 stars\":\n",
    "                    labels_list.append(\"Positif\")\n",
    "                elif elt[\"label\"] == \"3 stars\":\n",
    "                    labels_list.append(\"Neutre\")\n",
    "        score_list = []\n",
    "\n",
    "dataset[\"Labels\"] = labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des commentaires négatifs\n",
    "\n",
    "indices_to_keep = []\n",
    "indices_to_remove = []\n",
    "\n",
    "for indice in range(len(dataset[\"Labels\"])):\n",
    "    if dataset[\"Labels\"][indice] == \"Négatif\":\n",
    "        indices_to_keep.append(indice)\n",
    "        \n",
    "for index in range(len(dataset[\"Labels\"])):\n",
    "    if index not in indices_to_keep:\n",
    "        indices_to_remove.append(index)\n",
    "        \n",
    "dataset.drop(indices_to_remove, axis=0, inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de génération de topics\n",
    "def get_topic(words_list):\n",
    "    topic = [elt for elt in words_list.keys()]\n",
    "    topic = topic[:4]\n",
    "    topic = ' '.join(topic)\n",
    "    topic = topic.split()\n",
    "\n",
    "    mots_uniques = []\n",
    "\n",
    "    for mot in topic:\n",
    "        if mot not in mots_uniques:\n",
    "            mots_uniques.append(mot)\n",
    "\n",
    "    topic = ' '.join(mots_uniques)\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les topics à partir d'un objet WordCloud\n",
    "\n",
    "long_string = []\n",
    "topics_list = []\n",
    "\n",
    "for index in range(len(dataset[\"Processing\"])):\n",
    "    for word in dataset[\"Processing\"][index]:\n",
    "        long_string.append(word)\n",
    "        \n",
    "    strg = ','.join(elt for elt in long_string)\n",
    "\n",
    "    # Création d'un objet WordCloud\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "    # Générer le nuage de mots\n",
    "    wordcloud.generate(strg)\n",
    "\n",
    "    # Récupération des mots les plus importants\n",
    "    words_list = wordcloud.words_\n",
    "    \n",
    "    topics_list.append(get_topic(words_list))\n",
    "    \n",
    "    long_string = []\n",
    "\n",
    "dataset[\"Topics\"] = topics_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
